{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3BlYm0GKnuH",
        "outputId": "289af131-e570-4247-ddd4-606854c4c84b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/app'"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "#!pip -q install tensorflow   # currently 2.13.1\n",
        "#!pip install -U transformers"
      ],
      "metadata": {
        "id": "TOyVaq6r3oVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH     # need to add this to Dockerfile or execute in container to add CUDA libraries to the path in order to detect CUDA libraries\n",
        "%ls /usr/local/cuda/lib64/libcudart.so\n",
        "%ls /usr/local/cuda/lib64/libcudart.so.11.0\n",
        "%ls /usr/local/cuda/lib64/libcudart.so.12.0       # this was not installed!\n",
        "# %ls /usr/local/cuda/lib64/      # list of all CUDA libraries installed\n",
        "# export LD_LIBRARY_PATH=/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:$LD_LIBRARY_PATH\n",
        "# Update Dockerfile for CUDA environment variable, and apt-get update, upgrade, and then install -y cuda-libraries-11-6 cuda-libraries-dev-11-6 cuda-tools-11-6\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxqqZAh54UzP",
        "outputId": "549c9c8d-6fc2-453f-ac98-941a0ea47c76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;36m/usr/local/cuda/lib64/libcudart.so\u001b[0m@\n",
            "ls: cannot access '/usr/local/cuda/lib64/libcudart.so.11.0': No such file or directory\n",
            "ls: cannot access '/usr/local/cuda/lib64/libcudart.so.12.0': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#find / -name \"libcudart.so*\"     # running this in container to check conflicting versions of CUDA libraries gave the below result\n",
        "  #/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib/libcudart.so.12\n",
        "  #/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudart.so.11.6.55\n",
        "  #/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudart.so.11.0\n",
        "  #/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudart.so"
      ],
      "metadata": {
        "id": "o2tcwypb7EDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdVSk5iZ1DVB",
        "outputId": "b91626bf-645e-4a69-9d5b-7e978645f380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Sep  6 03:32:19 2024       \r\n",
            "+---------------------------------------------------------------------------------------+\r\n",
            "| NVIDIA-SMI 535.86.01              Driver Version: 536.67       CUDA Version: 12.2     |\r\n",
            "|-----------------------------------------+----------------------+----------------------+\r\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
            "|                                         |                      |               MIG M. |\r\n",
            "|=========================================+======================+======================|\r\n",
            "|   0  NVIDIA GeForce RTX 4070 Ti     On  | 00000000:01:00.0 Off |                  N/A |\r\n",
            "|  0%   32C    P8               5W / 285W |   8571MiB / 12282MiB |      0%      Default |\r\n",
            "|                                         |                      |                  N/A |\r\n",
            "+-----------------------------------------+----------------------+----------------------+\r\n",
            "                                                                                         \r\n",
            "+---------------------------------------------------------------------------------------+\r\n",
            "| Processes:                                                                            |\r\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
            "|        ID   ID                                                             Usage      |\r\n",
            "|=======================================================================================|\r\n",
            "|    0   N/A  N/A        32      C   /python3.8                                N/A      |\r\n",
            "+---------------------------------------------------------------------------------------+\r\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mistral 7B Instruct\n",
        "\n"
      ],
      "metadata": {
        "id": "H0shki19igLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ],
      "metadata": {
        "id": "kpXQGhHlij6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab4dc51-0f2f-4533-a0d7-033da71de9ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-09-05 23:22:47.982159: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-09-05 23:22:47.984315: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-09-05 23:22:48.020678: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-09-05 23:22:48.803179: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.set_default_device('cuda')      # works on fresh container state\n",
        "torch.cuda.set_device(0)                # works after installing torchvision and torchaudio since torch version is updated"
      ],
      "metadata": {
        "id": "NhvFmDhU1RQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log into HuggingFace\n",
        "from huggingface_hub import login\n",
        "login(\"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\")     # save to file later"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77yCAT5MI7pm",
        "outputId": "fc45631c-9e71-4d53-a37b-2799db3a7d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable verbose logging to see progress bar for downloading model from Hugging Face\n",
        "#from transformers.utils import logging\n",
        "#logging.set_verbosity_info()\n",
        "#logging.set_verbosity_error()   # only display error"
      ],
      "metadata": {
        "id": "2JFOHCR9xUlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "# !nvidia-smi\n",
        "print(\"CUDA availability:\",torch.cuda.is_available())\n",
        "print(\"Number of devices:\", torch.cuda.device_count())\n",
        "print(\"Device name:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msPMC7KsyYkF",
        "outputId": "d53d1867-32e2-496c-9ee1-a42742beeaa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA availability: True\n",
            "Number of devices: 1\n",
            "Device name: NVIDIA GeForce RTX 4070 Ti\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model.to(device)\n",
        "#device = torch.device(\"cuda\")\n",
        "#!nvcc --version\n",
        "#!pip install bitsandbytes==0.43.0\n",
        "#!wget https://files.pythonhosted.org/packages/72/e5/06ed351cdf8d1d5bf7eb86729e4e3669c6844654354aef3b1bc9da66d0bb/bitsandbytes-0.43.2-py3-none-manylinux_2_24_x86_64.whl   # bitsandbytes-0.43.2-py3-none-manylinux_2_24_x86_64.whl\n",
        "#!pip install bitsandbytes-0.43.2-py3-none-manylinux_2_24_x86_64.whl\n",
        "\n",
        "import platform\n",
        "print(platform.system())   # check system - Linux\n",
        "!uname -m                  # check architecture - x86_64\n",
        "!python3 --version\n",
        "!pip show bitsandbytes\n",
        "#!pip uninstall bitsandbytes -y\n",
        "#!pip install git+https://github.com/TimDettmers/bitsandbytes.git\n",
        "!pip list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arouvA33aLRS",
        "outputId": "a0a099ab-6d20-4dee-c478-e0530d2450d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linux\n",
            "x86_64\n",
            "Python 3.8.10\n",
            "Name: bitsandbytes\n",
            "Version: 0.43.3\n",
            "Summary: k-bit optimizers and matrix multiplication routines.\n",
            "Home-page: https://github.com/TimDettmers/bitsandbytes\n",
            "Author: Tim Dettmers\n",
            "Author-email: dettmers@cs.washington.edu\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.8/dist-packages\n",
            "Requires: torch, numpy\n",
            "Required-by: \n",
            "Package                                 Version     \n",
            "--------------------------------------- ------------\n",
            "absl-py                                 2.1.0       \n",
            "accelerate                              0.34.2      \n",
            "aiohappyeyeballs                        2.4.0       \n",
            "aiohttp                                 3.10.5      \n",
            "aiosignal                               1.3.1       \n",
            "annotated-types                         0.7.0       \n",
            "anyio                                   4.4.0       \n",
            "argon2-cffi                             23.1.0      \n",
            "argon2-cffi-bindings                    21.2.0      \n",
            "asttokens                               2.4.1       \n",
            "astunparse                              1.6.3       \n",
            "async-lru                               2.0.4       \n",
            "async-timeout                           4.0.3       \n",
            "attrs                                   24.2.0      \n",
            "babel                                   2.16.0      \n",
            "backcall                                0.2.0       \n",
            "beautifulsoup4                          4.12.3      \n",
            "bitsandbytes                            0.43.3      \n",
            "bleach                                  6.1.0       \n",
            "cachetools                              5.5.0       \n",
            "certifi                                 2024.8.30   \n",
            "cffi                                    1.17.1      \n",
            "charset-normalizer                      3.3.2       \n",
            "click                                   8.1.7       \n",
            "comm                                    0.2.2       \n",
            "contourpy                               1.1.1       \n",
            "cycler                                  0.12.1      \n",
            "dataclasses-json                        0.6.7       \n",
            "datasets                                2.21.0      \n",
            "debugpy                                 1.8.5       \n",
            "decorator                               5.1.1       \n",
            "defusedxml                              0.7.1       \n",
            "Deprecated                              1.2.14      \n",
            "dill                                    0.3.8       \n",
            "dirtyjson                               1.0.8       \n",
            "distro                                  1.9.0       \n",
            "einops                                  0.8.0       \n",
            "exceptiongroup                          1.2.2       \n",
            "executing                               2.1.0       \n",
            "fastjsonschema                          2.20.0      \n",
            "filelock                                3.15.4      \n",
            "flatbuffers                             24.3.25     \n",
            "fonttools                               4.53.1      \n",
            "frozenlist                              1.4.1       \n",
            "fsspec                                  2024.9.0    \n",
            "gast                                    0.4.0       \n",
            "google-auth                             2.34.0      \n",
            "google-auth-oauthlib                    1.0.0       \n",
            "google-pasta                            0.2.0       \n",
            "greenlet                                3.0.3       \n",
            "grpcio                                  1.66.1      \n",
            "h11                                     0.14.0      \n",
            "h5py                                    3.11.0      \n",
            "httpcore                                1.0.5       \n",
            "httpx                                   0.27.2      \n",
            "huggingface-hub                         0.24.6      \n",
            "idna                                    3.8         \n",
            "importlib-metadata                      8.4.0       \n",
            "importlib-resources                     6.4.4       \n",
            "ipykernel                               6.29.5      \n",
            "ipython                                 8.12.3      \n",
            "ipywidgets                              8.1.5       \n",
            "jedi                                    0.19.1      \n",
            "jinja2                                  3.1.4       \n",
            "jiter                                   0.5.0       \n",
            "joblib                                  1.4.2       \n",
            "json5                                   0.9.25      \n",
            "jsonpatch                               1.33        \n",
            "jsonpointer                             3.0.0       \n",
            "jsonschema                              4.23.0      \n",
            "jsonschema-specifications               2023.12.1   \n",
            "jupyter                                 1.1.1       \n",
            "jupyter-client                          8.6.2       \n",
            "jupyter-console                         6.6.3       \n",
            "jupyter-core                            5.7.2       \n",
            "jupyter-events                          0.10.0      \n",
            "jupyter-lsp                             2.2.5       \n",
            "jupyter-server                          2.14.2      \n",
            "jupyter-server-terminals                0.5.3       \n",
            "jupyterlab                              4.2.5       \n",
            "jupyterlab-pygments                     0.3.0       \n",
            "jupyterlab-server                       2.27.3      \n",
            "jupyterlab-widgets                      3.0.13      \n",
            "keras                                   2.13.1      \n",
            "kiwisolver                              1.4.7       \n",
            "langchain                               0.2.16      \n",
            "langchain-core                          0.2.38      \n",
            "langchain-text-splitters                0.2.4       \n",
            "langsmith                               0.1.115     \n",
            "libclang                                18.1.1      \n",
            "llama-cloud                             0.0.15      \n",
            "llama-index                             0.11.6      \n",
            "llama-index-agent-openai                0.3.0       \n",
            "llama-index-cli                         0.3.0       \n",
            "llama-index-core                        0.11.6      \n",
            "llama-index-embeddings-openai           0.2.4       \n",
            "llama-index-indices-managed-llama-cloud 0.3.0       \n",
            "llama-index-legacy                      0.9.48.post3\n",
            "llama-index-llms-openai                 0.2.2       \n",
            "llama-index-multi-modal-llms-openai     0.2.0       \n",
            "llama-index-program-openai              0.2.0       \n",
            "llama-index-question-gen-openai         0.2.0       \n",
            "llama-index-readers-file                0.2.1       \n",
            "llama-index-readers-llama-parse         0.3.0       \n",
            "llama-parse                             0.5.2       \n",
            "loralib                                 0.1.2       \n",
            "Markdown                                3.7         \n",
            "MarkupSafe                              2.1.5       \n",
            "marshmallow                             3.22.0      \n",
            "matplotlib                              3.7.5       \n",
            "matplotlib-inline                       0.1.7       \n",
            "mistune                                 3.0.2       \n",
            "mpmath                                  1.3.0       \n",
            "multidict                               6.0.5       \n",
            "multiprocess                            0.70.16     \n",
            "mypy-extensions                         1.0.0       \n",
            "nbclient                                0.10.0      \n",
            "nbconvert                               7.16.4      \n",
            "nbformat                                5.10.4      \n",
            "nest-asyncio                            1.6.0       \n",
            "networkx                                3.1         \n",
            "nltk                                    3.9.1       \n",
            "notebook                                7.2.2       \n",
            "notebook-shim                           0.2.4       \n",
            "numpy                                   1.24.4      \n",
            "nvidia-cublas-cu12                      12.1.3.1    \n",
            "nvidia-cuda-cupti-cu12                  12.1.105    \n",
            "nvidia-cuda-nvrtc-cu12                  12.1.105    \n",
            "nvidia-cuda-runtime-cu12                12.1.105    \n",
            "nvidia-cudnn-cu12                       9.1.0.70    \n",
            "nvidia-cufft-cu12                       11.0.2.54   \n",
            "nvidia-curand-cu12                      10.3.2.106  \n",
            "nvidia-cusolver-cu12                    11.4.5.107  \n",
            "nvidia-cusparse-cu12                    12.1.0.106  \n",
            "nvidia-nccl-cu12                        2.20.5      \n",
            "nvidia-nvjitlink-cu12                   12.6.68     \n",
            "nvidia-nvtx-cu12                        12.1.105    \n",
            "oauthlib                                3.2.2       \n",
            "openai                                  1.43.0      \n",
            "opt-einsum                              3.3.0       \n",
            "orjson                                  3.10.7      \n",
            "overrides                               7.7.0       \n",
            "packaging                               24.1        \n",
            "pandas                                  2.0.3       \n",
            "pandocfilters                           1.5.1       \n",
            "parso                                   0.8.4       \n",
            "pexpect                                 4.9.0       \n",
            "pickleshare                             0.7.5       \n",
            "pillow                                  10.4.0      \n",
            "pip                                     20.0.2      \n",
            "pkgutil-resolve-name                    1.3.10      \n",
            "platformdirs                            4.2.2       \n",
            "prometheus-client                       0.20.0      \n",
            "prompt-toolkit                          3.0.47      \n",
            "protobuf                                4.25.4      \n",
            "psutil                                  6.0.0       \n",
            "ptyprocess                              0.7.0       \n",
            "pure-eval                               0.2.3       \n",
            "pyarrow                                 17.0.0      \n",
            "pyasn1                                  0.6.0       \n",
            "pyasn1-modules                          0.4.0       \n",
            "pycparser                               2.22        \n",
            "pydantic                                2.9.0       \n",
            "pydantic-core                           2.23.2      \n",
            "pygments                                2.18.0      \n",
            "pyparsing                               3.1.4       \n",
            "pypdf                                   4.3.1       \n",
            "python-dateutil                         2.9.0.post0 \n",
            "python-json-logger                      2.0.7       \n",
            "pytz                                    2024.1      \n",
            "PyYAML                                  6.0.2       \n",
            "pyzmq                                   26.2.0      \n",
            "referencing                             0.35.1      \n",
            "regex                                   2024.7.24   \n",
            "requests                                2.32.3      \n",
            "requests-oauthlib                       2.0.0       \n",
            "rfc3339-validator                       0.1.4       \n",
            "rfc3986-validator                       0.1.1       \n",
            "rpds-py                                 0.20.0      \n",
            "rsa                                     4.9         \n",
            "safetensors                             0.4.5       \n",
            "scikit-learn                            1.3.2       \n",
            "scipy                                   1.10.1      \n",
            "Send2Trash                              1.8.3       \n",
            "sentencepiece                           0.2.0       \n",
            "setuptools                              45.2.0      \n",
            "six                                     1.16.0      \n",
            "sniffio                                 1.3.1       \n",
            "soupsieve                               2.6         \n",
            "SQLAlchemy                              2.0.34      \n",
            "stack-data                              0.6.3       \n",
            "striprtf                                0.0.26      \n",
            "sympy                                   1.13.2      \n",
            "tenacity                                8.5.0       \n",
            "tensorboard                             2.13.0      \n",
            "tensorboard-data-server                 0.7.2       \n",
            "tensorflow                              2.13.1      \n",
            "tensorflow-estimator                    2.13.0      \n",
            "tensorflow-io-gcs-filesystem            0.34.0      \n",
            "termcolor                               2.4.0       \n",
            "terminado                               0.18.1      \n",
            "threadpoolctl                           3.5.0       \n",
            "tiktoken                                0.7.0       \n",
            "tinycss2                                1.3.0       \n",
            "tokenizers                              0.19.1      \n",
            "tomli                                   2.0.1       \n",
            "torch                                   2.4.1       \n",
            "tornado                                 6.4.1       \n",
            "tqdm                                    4.66.5      \n",
            "traitlets                               5.14.3      \n",
            "transformers                            4.45.0.dev0 \n",
            "triton                                  3.0.0       \n",
            "typing-extensions                       4.12.2      \n",
            "typing-inspect                          0.9.0       \n",
            "tzdata                                  2024.1      \n",
            "urllib3                                 2.2.2       \n",
            "wcwidth                                 0.2.13      \n",
            "webencodings                            0.5.1       \n",
            "websocket-client                        1.8.0       \n",
            "werkzeug                                3.0.4       \n",
            "wheel                                   0.34.2      \n",
            "widgetsnbextension                      4.0.13      \n",
            "wrapt                                   1.16.0      \n",
            "xformers                                0.0.27.post2\n",
            "xxhash                                  3.5.0       \n",
            "yarl                                    1.9.11      \n",
            "zipp                                    3.20.1      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from accelerate import init_empty_weights, infer_auto_device_map\n",
        "\n",
        "# Initialize the model with no weights\n",
        "with init_empty_weights():\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")      ##### Might need to use the sharded version of the model for CPU offloading https://huggingface.co/google/flan-ul2/discussions/8\n",
        "\n",
        "\n",
        "\n",
        "# Create the device map to offload some layers to the CPU\n",
        "# Refer to https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n",
        "device_map = infer_auto_device_map(model, max_memory={\"cpu\": \"48GB\", 0: \"12GB\"})     # i7-12700k has 128 GB memory; give the system some buffer room to avoid CUDA memory error by assigning 11.5 instead of 12\n",
        "\n",
        "# Load the model with the device map\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "                                             load_in_8bit=True,\n",
        "                                             device_map=device_map,\n",
        "                                             torch_dtype=\"auto\")\n",
        "\"\"\"\n",
        "\n",
        "# Load the model. Optimize via gradient checkpointing, 8-bit quantization, and automatic device mapping\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "                                             load_in_8bit=True,   # 8-bit quantization - Quantizing the model can significantly reduce its memory footprint - leverage BitsAndBytes for 8-bit or 4-bit quantization\n",
        "                                                                  # Loads the model in 8-bit precision, reducing memory footprint by ~1/2 compared to full precision (16-bit or 32-bit)\n",
        "                                                                  # load_in_4bit=True reduces it further, but comes with slight trade-off in model performance\n",
        "                                             device_map=\"auto\",   # Automatic device mapping - automatically splits the model layers across available devices (e.g., GPU and CPU) depending on available memory\n",
        "                                             torch_dtype=\"auto\")\n",
        "\n",
        "\n",
        "# Gradient checkpointing - reduces memory usage during the forward pass by recomputing certain parts of the graph - i.e., activations - in the backward pass instead of storing intermediate states\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "                                          torch_dtype=\"auto\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35,
          "referenced_widgets": [
            "8819a475db2045d2b3d8dac242919801",
            "9d783bc4c8214eae8d00bbafb5b15648",
            "70f960eb0b274ae9af704ca5954817af",
            "4c4e027df1e8417596df3ce8c0fd7607",
            "bfd29ef04ff84097911292dbe9199ced",
            "99259e26917d4451989b3b657938f37f",
            "a979263055b546368462af1d7b4c4cd8",
            "26b9986c4ca84ac0b5e756958d1704ca",
            "3afa8942eec04e36b2512fd2f767c175",
            "47ddc739aaba4764969106545ffbb259",
            "c26080549c0c474a890f5d1cdce9fe21"
          ]
        },
        "id": "WsYot3Rz1NVf",
        "outputId": "111553dc-9ab9-4bab-e128-63b862a1a42b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8819a475db2045d2b3d8dac242919801"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"<s>[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST]\"\n",
        "\n",
        "encodeds = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "\n",
        "device = 'cuda'\n",
        "model_inputs = encodeds.to(device)\n",
        "#model.to(device)     # to.() not supported for 8-bit. Just use model as is, since correct device has already been assigned to it\n",
        "\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ],
      "metadata": {
        "id": "Bm3FAkWVVmJV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0afb890f-5853-4fff-e627-874dcf6165dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Do you have mayonnaise recipes? [/INST] Of course! Mayonnaise is a classic condiment that's easy to make at home. Here's a simple recipe that you can try out:\n",
            "\n",
            "Ingredients:\n",
            "\n",
            "* 2 egg yolks\n",
            "* 1 teaspoon dijon mustard\n",
            "* 2 tablespoons white wine vinegar or lemon juice\n",
            "* 1 tablespoon honey or maple syrup\n",
            "* 1/2 cup vegetable or canola oil\n",
            "* Salt and pepper to taste\n",
            "\n",
            "Instructions:\n",
            "\n",
            "1. In a medium-sized bowl, whisk together the egg yolks, mustard, vinegar, honey, salt, and pepper.\n",
            "2. Slowly add the oil to the egg mixture, whisking continuously until the oil is well emulsified and the mixture thickens.\n",
            "3. Taste and adjust the seasoning as needed.\n",
            "4. Cover the bowl with a clean kitchen towel or wrap with plastic wrap and refrigerate for at least an hour to allow the flavors to meld.\n",
            "5. Once chilled, give the mayonnaise another whisk before serving.\n",
            "\n",
            "That's it! You now have a homemade batch of delicious mayonnaise that's perfect for sandwiches, salads, and dipping. Enjoy!</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practice prompt\n",
        "text = \"\"\"<s>[INST] What do you think are the most important things in life? [/INST]\n",
        "          I think building healthy, long-lasting relationships, a fulfilling career and exciting hobbies are vital components for a happy life!</s>\n",
        "          [INST] Do you have any life advice to give someone in their early thirties seeking a career transition to machine learning, particularly in LLM's? [/INST]\"\"\"\n",
        "\n",
        "encodeds = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "\n",
        "device = 'cuda'\n",
        "model_inputs = encodeds.to(device)\n",
        "#model.to(device)     # to.() not supported for 8-bit. Just use model as is, since correct device has already been assigned to it\n",
        "\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-K9YOsx_t77",
        "outputId": "c036e733-48ed-448f-caec-e440c192612e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] What do you think are the most important things in life? [/INST]\n",
            "          I think building healthy, long-lasting relationships, a fulfilling career and exciting hobbies are vital components for a happy life!</s>\n",
            "          [INST] Do you have any life advice to give someone in their early thirties seeking a career transition to machine learning, particularly in LLM's? [/INST] Sure, here are some tips to help you in your career transition to machine learning:\n",
            "\n",
            "1. Brush up on your math and programming skills. Machine learning relies heavily on math, such as linear algebra and calculus, and programming skills such as Python, R, or MATLAB.\n",
            "2. Learn the basics of machine learning algorithms and techniques. Familiarize yourself with the various types of machine learning problems and common methods for solving them.\n",
            "3. Look into online courses and certifications that can help you learn the necessary skills faster. There are many platforms such as Coursera, Udacity, and EdX that offer online courses on machine learning.\n",
            "4. Build a strong portfolio to showcase your skills. You could work on projects that allow you to apply your machine learning knowledge and demonstrate your abilities to potential employers.\n",
            "5. Network with other professionals in the field, and try to get involved with any relevant communities or organizations. This will help you connect with others who can provide valuable insights and advice.\n",
            "6. Be prepared for a potential decrease in salary during the transition. It may take time to acquire the necessary skills and gain experience in the field of machine learning. You should be prepared for the financial challenges that may arise during this process.\n",
            "7. Stay positive and motivated. Transitioning to a new career can be challenging, but with persistence and dedication, you can achieve your goals and make a successful career change.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Langchain"
      ],
      "metadata": {
        "id": "ext4TbwbMFLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define pipeline\n",
        "pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        use_cache=True,\n",
        "        device_map=\"auto\",\n",
        "        max_length=500,\n",
        "        do_sample=True,\n",
        "        top_k=5,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        ")"
      ],
      "metadata": {
        "id": "eMcNhxlsHxcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain-community langchain-core\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "llm = HuggingFacePipeline(pipeline=pipeline)     # connect Langchain to defined pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZa7iY8HHxmE",
        "outputId": "103a1ace-62b3-4594-c81a-ca3d10d9d34b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_32/3287174287.py:4: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
            "  llm = HuggingFacePipeline(pipeline=pipeline)     # connect Langchain to defined pipeline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the same prompt based on given pipeline\n",
        "%%time\n",
        "template = \"\"\"<s>[INST] What do you think are the most important things in life? [/INST]\n",
        "          I think building healthy, long-lasting relationships, a fulfilling career and exciting hobbies are vital components for a happy life!</s>\n",
        "          [INST] Do you have any life advice to give someone in their early thirties seeking a career transition to machine learning, particularly in LLM's? [/INST]\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"\",\"\"])\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "response = llm_chain.run({\"question\":\"\",\"context\":\"\"})\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJZX36dsHxsn",
        "outputId": "bff598fc-a268-4ab2-8fde-b7ec53fa5db7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<timed exec>:6: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "<timed exec>:7: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 2s, sys: 7.18 s, total: 1min 9s\n",
            "Wall time: 1min 9s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<s>[INST] What do you think are the most important things in life? [/INST]\\n          I think building healthy, long-lasting relationships, a fulfilling career and exciting hobbies are vital components for a happy life!</s>\\n          [INST] Do you have any life advice to give someone in their early thirties seeking a career transition to machine learning, particularly in LLM's? [/INST] Here are some life advice for someone in their early thirties seeking a career transition to machine learning, particularly in LLM's:\\n\\n1. Start by gaining a strong foundation in mathematics and computer science. Machine learning requires a solid understanding of linear algebra, calculus, probability, and programming languages such as Python.\\n2. Learn about the field of natural language processing (NLP) and how it relates to machine learning. LLM's are a subfield of NLP and require a deep understanding of the language itself, as well as techniques for working with text data.\\n3. Get involved in machine learning projects and competitions to build your portfolio and gain practical experience.\\n4. Consider pursuing advanced degrees such as a Master's or Ph.D. in a related field. These can provide valuable research experience and help you build a network of contacts in the industry.\\n5. Network with professionals in the field by attending conferences, workshops and joining online communities and forums.\\n6. Be patient and persistent, the field of machine learning is constantly evolving and new challenges are always arising, it may take time to gain the necessary skills and experience to succeed.\""
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test new prompt with different format\n",
        "%%time\n",
        "template = \"\"\"<s>[INST] You are a helpful, respectful and honest assistant. Always provide your answer politely and exactly.\n",
        "              Answer the question below from the following context:\n",
        "              {context}\n",
        "              {question} [/INST] </s>\n",
        "            \"\"\"\n",
        "\n",
        "question_p = \"\"\"How many apples does Jordan have?\"\"\"\n",
        "context_p = \"\"\" Upon waking up, Jordan noticed that apples were raining from the sky. In dire need of vitamin C, Jordan swiftly went up to the roof of his house with a giant net in hand, and collected fifty-two of those heavenly fruits.\n",
        "                However, he noticed while washing them that many of them became inedible from the fall and tossed out 26 of them. Unable to fight off his hunger any longer, he ate one for himself.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\",\"context\"])\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "response = llm_chain.run({\"question\":question_p,\"context\":context_p})\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq3ehheUHxu8",
        "outputId": "5cd27b83-0fbf-47e4-e741-79995e590ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 14.4 s, sys: 557 ms, total: 15 s\n",
            "Wall time: 15 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s>[INST] You are a helpful, respectful and honest assistant. Always provide your answer politely and exactly.\\n              Answer the question below from the following context:\\n               Upon waking up, Jordan noticed that apples were raining from the sky. In dire need of vitamin C, Jordan swiftly went up to the roof of his house with a giant net in hand, and collected fifty-two of those heavenly fruits.\\n                However, he noticed while washing them that many of them became inedible from the fall and tossed out 26 of them. Unable to fight off his hunger any longer, he ate one for himself.\\n              How many apples does Jordan have? [/INST] </s>\\n             Jordan has 25 apples left after tossing out 26 inedible fruits.'"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. RAG"
      ],
      "metadata": {
        "id": "0i2Imz7WMIzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install chromadb #==0.3.29         #  Chroma requires sqlite3 >= 3.35.0\n",
        "#!pip install pysqlite3-binary\n",
        "__import__('pysqlite3')\n",
        "import sys\n",
        "#sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
        "#!pip install sentence-transformers\n",
        "import pysqlite3\n",
        "sys.modules['sqlite3'] = sys.modules[\"pysqlite3\"]\n",
        "\n",
        "import chromadb           # import chromadb after making changes to pysqlite3\n",
        "from chromadb.config import Settings\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "uwocTBJuHxxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall chromadb -y\n",
        "#!pip install chromadb==0.5.3\n",
        "!pip list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2X75A12GXiFL",
        "outputId": "b308865f-37a3-4aff-903a-c36fe11e36cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package                                  Version     \r\n",
            "---------------------------------------- ------------\r\n",
            "absl-py                                  2.1.0       \r\n",
            "accelerate                               0.34.2      \r\n",
            "aiohappyeyeballs                         2.4.0       \r\n",
            "aiohttp                                  3.10.5      \r\n",
            "aiosignal                                1.3.1       \r\n",
            "annotated-types                          0.7.0       \r\n",
            "anyio                                    4.4.0       \r\n",
            "argon2-cffi                              23.1.0      \r\n",
            "argon2-cffi-bindings                     21.2.0      \r\n",
            "asgiref                                  3.8.1       \r\n",
            "asttokens                                2.4.1       \r\n",
            "astunparse                               1.6.3       \r\n",
            "async-lru                                2.0.4       \r\n",
            "async-timeout                            4.0.3       \r\n",
            "attrs                                    24.2.0      \r\n",
            "babel                                    2.16.0      \r\n",
            "backcall                                 0.2.0       \r\n",
            "backoff                                  2.2.1       \r\n",
            "bcrypt                                   4.2.0       \r\n",
            "beautifulsoup4                           4.12.3      \r\n",
            "bitsandbytes                             0.43.3      \r\n",
            "bleach                                   6.1.0       \r\n",
            "build                                    1.2.1       \r\n",
            "cachetools                               5.5.0       \r\n",
            "certifi                                  2024.8.30   \r\n",
            "cffi                                     1.17.1      \r\n",
            "charset-normalizer                       3.3.2       \r\n",
            "chroma-hnswlib                           0.7.3       \r\n",
            "chromadb                                 0.5.3       \r\n",
            "click                                    8.1.7       \r\n",
            "clickhouse-connect                       0.7.19      \r\n",
            "coloredlogs                              15.0.1      \r\n",
            "comm                                     0.2.2       \r\n",
            "contourpy                                1.1.1       \r\n",
            "cycler                                   0.12.1      \r\n",
            "dataclasses-json                         0.6.7       \r\n",
            "datasets                                 2.21.0      \r\n",
            "debugpy                                  1.8.5       \r\n",
            "decorator                                5.1.1       \r\n",
            "defusedxml                               0.7.1       \r\n",
            "Deprecated                               1.2.14      \r\n",
            "dill                                     0.3.8       \r\n",
            "dirtyjson                                1.0.8       \r\n",
            "distro                                   1.9.0       \r\n",
            "duckdb                                   1.0.0       \r\n",
            "einops                                   0.8.0       \r\n",
            "exceptiongroup                           1.2.2       \r\n",
            "executing                                2.1.0       \r\n",
            "fastapi                                  0.113.0     \r\n",
            "fastjsonschema                           2.20.0      \r\n",
            "filelock                                 3.15.4      \r\n",
            "flatbuffers                              24.3.25     \r\n",
            "fonttools                                4.53.1      \r\n",
            "frozenlist                               1.4.1       \r\n",
            "fsspec                                   2024.9.0    \r\n",
            "gast                                     0.4.0       \r\n",
            "google-auth                              2.34.0      \r\n",
            "google-auth-oauthlib                     1.0.0       \r\n",
            "google-pasta                             0.2.0       \r\n",
            "googleapis-common-protos                 1.65.0      \r\n",
            "graphlib-backport                        1.1.0       \r\n",
            "greenlet                                 3.0.3       \r\n",
            "grpcio                                   1.66.1      \r\n",
            "h11                                      0.14.0      \r\n",
            "h5py                                     3.11.0      \r\n",
            "hnswlib                                  0.8.0       \r\n",
            "httpcore                                 1.0.5       \r\n",
            "httptools                                0.6.1       \r\n",
            "httpx                                    0.27.2      \r\n",
            "huggingface-hub                          0.24.6      \r\n",
            "humanfriendly                            10.0        \r\n",
            "idna                                     3.8         \r\n",
            "importlib-metadata                       8.4.0       \r\n",
            "importlib-resources                      6.4.4       \r\n",
            "ipykernel                                6.29.5      \r\n",
            "ipython                                  8.12.3      \r\n",
            "ipywidgets                               8.1.5       \r\n",
            "jedi                                     0.19.1      \r\n",
            "jinja2                                   3.1.4       \r\n",
            "jiter                                    0.5.0       \r\n",
            "joblib                                   1.4.2       \r\n",
            "json5                                    0.9.25      \r\n",
            "jsonpatch                                1.33        \r\n",
            "jsonpointer                              3.0.0       \r\n",
            "jsonschema                               4.23.0      \r\n",
            "jsonschema-specifications                2023.12.1   \r\n",
            "jupyter                                  1.1.1       \r\n",
            "jupyter-client                           8.6.2       \r\n",
            "jupyter-console                          6.6.3       \r\n",
            "jupyter-core                             5.7.2       \r\n",
            "jupyter-events                           0.10.0      \r\n",
            "jupyter-lsp                              2.2.5       \r\n",
            "jupyter-server                           2.14.2      \r\n",
            "jupyter-server-terminals                 0.5.3       \r\n",
            "jupyterlab                               4.2.5       \r\n",
            "jupyterlab-pygments                      0.3.0       \r\n",
            "jupyterlab-server                        2.27.3      \r\n",
            "jupyterlab-widgets                       3.0.13      \r\n",
            "keras                                    2.13.1      \r\n",
            "kiwisolver                               1.4.7       \r\n",
            "kubernetes                               30.1.0      \r\n",
            "langchain                                0.2.16      \r\n",
            "langchain-community                      0.2.16      \r\n",
            "langchain-core                           0.2.38      \r\n",
            "langchain-text-splitters                 0.2.4       \r\n",
            "langsmith                                0.1.115     \r\n",
            "libclang                                 18.1.1      \r\n",
            "llama-cloud                              0.0.15      \r\n",
            "llama-index                              0.11.6      \r\n",
            "llama-index-agent-openai                 0.3.0       \r\n",
            "llama-index-cli                          0.3.0       \r\n",
            "llama-index-core                         0.11.6      \r\n",
            "llama-index-embeddings-openai            0.2.4       \r\n",
            "llama-index-indices-managed-llama-cloud  0.3.0       \r\n",
            "llama-index-legacy                       0.9.48.post3\r\n",
            "llama-index-llms-openai                  0.2.2       \r\n",
            "llama-index-multi-modal-llms-openai      0.2.0       \r\n",
            "llama-index-program-openai               0.2.0       \r\n",
            "llama-index-question-gen-openai          0.2.0       \r\n",
            "llama-index-readers-file                 0.2.1       \r\n",
            "llama-index-readers-llama-parse          0.3.0       \r\n",
            "llama-parse                              0.5.2       \r\n",
            "loralib                                  0.1.2       \r\n",
            "lz4                                      4.3.3       \r\n",
            "Markdown                                 3.7         \r\n",
            "markdown-it-py                           3.0.0       \r\n",
            "MarkupSafe                               2.1.5       \r\n",
            "marshmallow                              3.22.0      \r\n",
            "matplotlib                               3.7.5       \r\n",
            "matplotlib-inline                        0.1.7       \r\n",
            "mdurl                                    0.1.2       \r\n",
            "mistune                                  3.0.2       \r\n",
            "mmh3                                     4.1.0       \r\n",
            "monotonic                                1.6         \r\n",
            "mpmath                                   1.3.0       \r\n",
            "multidict                                6.0.5       \r\n",
            "multiprocess                             0.70.16     \r\n",
            "mypy-extensions                          1.0.0       \r\n",
            "nbclient                                 0.10.0      \r\n",
            "nbconvert                                7.16.4      \r\n",
            "nbformat                                 5.10.4      \r\n",
            "nest-asyncio                             1.6.0       \r\n",
            "networkx                                 3.1         \r\n",
            "nltk                                     3.9.1       \r\n",
            "notebook                                 7.2.2       \r\n",
            "notebook-shim                            0.2.4       \r\n",
            "numpy                                    1.24.4      \r\n",
            "nvidia-cublas-cu12                       12.1.3.1    \r\n",
            "nvidia-cuda-cupti-cu12                   12.1.105    \r\n",
            "nvidia-cuda-nvrtc-cu12                   12.1.105    \r\n",
            "nvidia-cuda-runtime-cu12                 12.1.105    \r\n",
            "nvidia-cudnn-cu12                        9.1.0.70    \r\n",
            "nvidia-cufft-cu12                        11.0.2.54   \r\n",
            "nvidia-curand-cu12                       10.3.2.106  \r\n",
            "nvidia-cusolver-cu12                     11.4.5.107  \r\n",
            "nvidia-cusparse-cu12                     12.1.0.106  \r\n",
            "nvidia-nccl-cu12                         2.20.5      \r\n",
            "nvidia-nvjitlink-cu12                    12.6.68     \r\n",
            "nvidia-nvtx-cu12                         12.1.105    \r\n",
            "oauthlib                                 3.2.2       \r\n",
            "onnxruntime                              1.16.3      \r\n",
            "openai                                   1.43.0      \r\n",
            "opentelemetry-api                        1.27.0      \r\n",
            "opentelemetry-exporter-otlp-proto-common 1.27.0      \r\n",
            "opentelemetry-exporter-otlp-proto-grpc   1.27.0      \r\n",
            "opentelemetry-instrumentation            0.48b0      \r\n",
            "opentelemetry-instrumentation-asgi       0.48b0      \r\n",
            "opentelemetry-instrumentation-fastapi    0.48b0      \r\n",
            "opentelemetry-proto                      1.27.0      \r\n",
            "opentelemetry-sdk                        1.27.0      \r\n",
            "opentelemetry-semantic-conventions       0.48b0      \r\n",
            "opentelemetry-util-http                  0.48b0      \r\n",
            "opt-einsum                               3.3.0       \r\n",
            "orjson                                   3.10.7      \r\n",
            "overrides                                7.7.0       \r\n",
            "packaging                                24.1        \r\n",
            "pandas                                   2.0.3       \r\n",
            "pandocfilters                            1.5.1       \r\n",
            "parso                                    0.8.4       \r\n",
            "pexpect                                  4.9.0       \r\n",
            "pickleshare                              0.7.5       \r\n",
            "pillow                                   10.4.0      \r\n",
            "pip                                      20.0.2      \r\n",
            "pkgutil-resolve-name                     1.3.10      \r\n",
            "platformdirs                             4.2.2       \r\n",
            "posthog                                  3.6.3       \r\n",
            "prometheus-client                        0.20.0      \r\n",
            "prompt-toolkit                           3.0.47      \r\n",
            "protobuf                                 4.25.4      \r\n",
            "psutil                                   6.0.0       \r\n",
            "ptyprocess                               0.7.0       \r\n",
            "pulsar-client                            3.5.0       \r\n",
            "pure-eval                                0.2.3       \r\n",
            "pyarrow                                  17.0.0      \r\n",
            "pyasn1                                   0.6.0       \r\n",
            "pyasn1-modules                           0.4.0       \r\n",
            "pycparser                                2.22        \r\n",
            "pydantic                                 1.10.18     \r\n",
            "pydantic-core                            2.23.2      \r\n",
            "pygments                                 2.18.0      \r\n",
            "pyparsing                                3.1.4       \r\n",
            "pypdf                                    4.3.1       \r\n",
            "PyPika                                   0.48.9      \r\n",
            "pyproject-hooks                          1.1.0       \r\n",
            "pysqlite3-binary                         0.5.3.post1 \r\n",
            "python-dateutil                          2.9.0.post0 \r\n",
            "python-dotenv                            1.0.1       \r\n",
            "python-json-logger                       2.0.7       \r\n",
            "pytz                                     2024.1      \r\n",
            "PyYAML                                   6.0.2       \r\n",
            "pyzmq                                    26.2.0      \r\n",
            "referencing                              0.35.1      \r\n",
            "regex                                    2024.7.24   \r\n",
            "requests                                 2.32.3      \r\n",
            "requests-oauthlib                        2.0.0       \r\n",
            "rfc3339-validator                        0.1.4       \r\n",
            "rfc3986-validator                        0.1.1       \r\n",
            "rich                                     13.8.0      \r\n",
            "rpds-py                                  0.20.0      \r\n",
            "rsa                                      4.9         \r\n",
            "safetensors                              0.4.5       \r\n",
            "scikit-learn                             1.3.2       \r\n",
            "scipy                                    1.10.1      \r\n",
            "Send2Trash                               1.8.3       \r\n",
            "sentence-transformers                    3.0.1       \r\n",
            "sentencepiece                            0.2.0       \r\n",
            "setuptools                               45.2.0      \r\n",
            "shellingham                              1.5.4       \r\n",
            "six                                      1.16.0      \r\n",
            "sniffio                                  1.3.1       \r\n",
            "soupsieve                                2.6         \r\n",
            "SQLAlchemy                               2.0.34      \r\n",
            "stack-data                               0.6.3       \r\n",
            "starlette                                0.38.4      \r\n",
            "striprtf                                 0.0.26      \r\n",
            "sympy                                    1.13.2      \r\n",
            "tenacity                                 8.5.0       \r\n",
            "tensorboard                              2.13.0      \r\n",
            "tensorboard-data-server                  0.7.2       \r\n",
            "tensorflow                               2.13.1      \r\n",
            "tensorflow-estimator                     2.13.0      \r\n",
            "tensorflow-io-gcs-filesystem             0.34.0      \r\n",
            "termcolor                                2.4.0       \r\n",
            "terminado                                0.18.1      \r\n",
            "threadpoolctl                            3.5.0       \r\n",
            "tiktoken                                 0.7.0       \r\n",
            "tinycss2                                 1.3.0       \r\n",
            "tokenizers                               0.19.1      \r\n",
            "tomli                                    2.0.1       \r\n",
            "torch                                    2.4.1       \r\n",
            "tornado                                  6.4.1       \r\n",
            "tqdm                                     4.66.5      \r\n",
            "traitlets                                5.14.3      \r\n",
            "transformers                             4.45.0.dev0 \r\n",
            "triton                                   3.0.0       \r\n",
            "typer                                    0.12.5      \r\n",
            "typing-extensions                        4.12.2      \r\n",
            "typing-inspect                           0.9.0       \r\n",
            "tzdata                                   2024.1      \r\n",
            "urllib3                                  2.2.2       \r\n",
            "uvicorn                                  0.30.6      \r\n",
            "uvloop                                   0.20.0      \r\n",
            "watchfiles                               0.24.0      \r\n",
            "wcwidth                                  0.2.13      \r\n",
            "webencodings                             0.5.1       \r\n",
            "websocket-client                         1.8.0       \r\n",
            "websockets                               13.0.1      \r\n",
            "werkzeug                                 3.0.4       \r\n",
            "wheel                                    0.34.2      \r\n",
            "widgetsnbextension                       4.0.13      \r\n",
            "wrapt                                    1.16.0      \r\n",
            "xformers                                 0.0.27.post2\r\n",
            "xxhash                                   3.5.0       \r\n",
            "yarl                                     1.9.11      \r\n",
            "zipp                                     3.20.1      \r\n",
            "zstandard                                0.23.0      \r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.skysports.com/tennis/news/32833/13209759/us-open-jack-draper-books-semi-final-spot-with-straight-sets-win-over-alex-de-minaur-at-flushing-meadows\n",
        "tennis_news = \"\"\"Jack Draper is through to the semi-finals of the US Open, with the British No 1 making light work of 10th seed Alex de Minaur 6-3 7-5 6-2 to set up a date against good friend and world No 1 Jannik Sinner - live on Sky Sports from 8pm, Friday, September 6, 2024.\n",
        "Draper's victory means he is the first British man to reach the final four at Flushing Meadows since Andy Murray won the tournament in 2012. The 22-year-old will next face top seed Sinner after the Italian defeated 2021 US Open champion Daniil Medvedev 6-2 1-6 6-1 6-4 on Wednesday night in New York.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hi5dU6c_Hxzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.document import Document\n",
        "documents = [Document(page_content=tennis_news, metadata={\"source\": \"local\"})]\n",
        "#######################\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
        "all_splits = text_splitter.split_documents(documents)\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "#######################\n",
        "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")\n",
        "#######################\n",
        "retriever = vectordb.as_retriever()\n",
        "#######################\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "def run_my_rag(qa, query):\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    result = qa.run(query)\n",
        "    print(\"\\nResult: \", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEcAh0Z0Hx1t",
        "outputId": "3e763ba1-3141-49dd-ead6-3348feae7d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:1602: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query =\"\"\" Which player do you think is the underdog based on this article? \"\"\"\n",
        "run_my_rag(qa, query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFTYuNYEMbg2",
        "outputId": "0bcc8e39-0b3a-447b-df03-5f826afe3f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:  Which player is the underdog? \n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Result:  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "Draper's victory means he is the first British man to reach the final four at Flushing Meadows since Andy Murray won the tournament in 2012. The 22-year-old will next face top seed Sinner after the Italian defeated 2021 US Open champion Daniil Medvedev 6-2 1-6 6-1 6-4 on Wednesday night in New York.\n",
            "\n",
            "Draper's victory means he is the first British man to reach the final four at Flushing Meadows since Andy Murray won the tournament in 2012. The 22-year-old will next face top seed Sinner after the Italian defeated 2021 US Open champion Daniil Medvedev 6-2 1-6 6-1 6-4 on Wednesday night in New York.\n",
            "\n",
            "JJack Draper is through to the semi-finals of the US Open, with the British No 1 making light work of 10th seed Alex de Minaur 6-3 7-5 6-2 to set up a date against good friend and world No 1 Jannik Sinner - live on Sky Sports from 8pm, Friday, September 6, 2024.\n",
            "\n",
            "JJack Draper is through to the semi-finals of the US Open, with the British No 1 making light work of 10th seed Alex de Minaur 6-3 7-5 6-2 to set up a date against good friend and world No 1 Jannik Sinner - live on Sky Sports from 8pm, Friday, September 6.\n",
            "\n",
            "Question:  Which player is the underdog? \n",
            "Helpful Answer: Jannik Sinner is the underdog. He is the world No 1 player while JJack Draper is ranked 105th in the world.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"What was the date on which Sinner defeated Medvedev?\"\"\"\n",
        "run_my_rag(qa, query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBuSr2NHMbju",
        "outputId": "f386ca75-3da0-4d92-ee69-a05e55409695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What was the date on which Sinner defeated Medvedev?\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Result:  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "Draper's victory means he is the first British man to reach the final four at Flushing Meadows since Andy Murray won the tournament in 2012. The 22-year-old will next face top seed Sinner after the Italian defeated 2021 US Open champion Daniil Medvedev 6-2 1-6 6-1 6-4 on Wednesday night in New York.\n",
            "\n",
            "Draper's victory means he is the first British man to reach the final four at Flushing Meadows since Andy Murray won the tournament in 2012. The 22-year-old will next face top seed Sinner after the Italian defeated 2021 US Open champion Daniil Medvedev 6-2 1-6 6-1 6-4 on Wednesday night in New York.\n",
            "\n",
            "JJack Draper is through to the semi-finals of the US Open, with the British No 1 making light work of 10th seed Alex de Minaur 6-3 7-5 6-2 to set up a date against good friend and world No 1 Jannik Sinner - live on Sky Sports from 8pm, Friday, September 6, 2024.\n",
            "\n",
            "JJack Draper is through to the semi-finals of the US Open, with the British No 1 making light work of 10th seed Alex de Minaur 6-3 7-5 6-2 to set up a date against good friend and world No 1 Jannik Sinner - live on Sky Sports from 8pm, Friday, September 6.\n",
            "\n",
            "Question: What was the date on which Sinner defeated Medvedev?\n",
            "Helpful Answer: Wednesday night (September 4, 2024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del pipeline\n",
        "del llm\n",
        "del model_4bit\n",
        "del quantization_config\n",
        "del tokenizer"
      ],
      "metadata": {
        "id": "9V73LdwIMbnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "krY9dPszMbsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Llama-Index"
      ],
      "metadata": {
        "id": "1nKLFCboTJD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install llama-index-readers-web\n",
        "#!pip install llama-index-llms-huggingface\n",
        "#!pip install llama-index-embeddings-huggingface\n",
        "#!pip install llama-index-agent-openai"
      ],
      "metadata": {
        "id": "tpKKSz_lTRaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "from llama_index.readers.web import BeautifulSoupWebReader\n",
        "\n",
        "url = \"https://www.theverge.com/2024/9/5/24235671/anker-eufy-smart-home-ifa-permanent-outdoor-lights-e22-e10\"\n",
        "\n",
        "documents = BeautifulSoupWebReader().load_data([url])"
      ],
      "metadata": {
        "id": "AMpMP411THxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip show pydantic     # 2.9.0\n",
        "#!pip show llama_index  # 0.11.6\n",
        "#!pip install pydantic==1.10.12      # llama-index-core 0.11.6 has requirement pydantic<3.0.0,>=2.7.0,\n",
        "#!pip install llama_index --upgrade\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVZ2QO8ETS0r",
        "outputId": "e8a595e8-bac2-4567-e5a5-630bbd7260d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydantic==1.10.12\r\n",
            "  Using cached pydantic-1.10.12-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic==1.10.12) (4.12.2)\n",
            "\u001b[31mERROR: text-generation 0.7.0 has requirement pydantic<3,>2, but you'll have pydantic 1.10.12 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: llama-index-core 0.11.6 has requirement pydantic<3.0.0,>=2.7.0, but you'll have pydantic 1.10.12 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pydantic\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.9.0\n",
            "    Uninstalling pydantic-2.9.0:\n",
            "      Successfully uninstalled pydantic-2.9.0\n",
            "Successfully installed pydantic-1.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bedLta5STHzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N7k-NNX0TH1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u2i2XMJiTH4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NuhXAQWuTH6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yzjJX_vAMbvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nFGJ3RciMbxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Format\n",
        "```\n",
        "<s>[INST] <user_prompt> [/INST]\n",
        "\n",
        "<assistant_response> </s>\n",
        "\n",
        "[INST] <user_prompt>[/INST]\n",
        "```"
      ],
      "metadata": {
        "id": "w9ykaBB_FF05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text(text, width=90): #preserve_newlines\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text"
      ],
      "metadata": {
        "id": "ocdzRuhhUTjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bYUiuyCWFE3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(input_text, system_prompt=\"\",max_length=512):\n",
        "    prompt = f\"\"\"<s>[INST]{input_text}[/INST]\"\"\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "    model_inputs = encodeds.to(device)\n",
        "    model.to(device)\n",
        "    outputs = model.generate(**inputs,\n",
        "                             max_length=max_length,\n",
        "                             temperature=0.1,\n",
        "                             do_sample=True)\n",
        "    text = tokenizer.batch_decode(outputs)[0]\n",
        "    wrapped_text = wrap_text(text)\n",
        "    print(wrapped_text)"
      ],
      "metadata": {
        "id": "E4mjX62HFPIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CodeGen"
      ],
      "metadata": {
        "id": "mlrwEjmvLZ1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate('''```python\n",
        "def print_prime(n):\n",
        "   \"\"\"\n",
        "   Print all primes between 1 and n\n",
        "   \"\"\"''', system_prompt=\"You are a genius python coder\")"
      ],
      "metadata": {
        "id": "w0M8bf36Cj4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate('''```python\n",
        "def detect_prime(n):\n",
        "   \"\"\"\n",
        "   detect if a number is a prime number or not. return True or False\n",
        "   \"\"\"''')"
      ],
      "metadata": {
        "id": "sGJbmCoYLfYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instruction Answering"
      ],
      "metadata": {
        "id": "_nhYTllvLTuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate('Write a detailed analogy between mathematics and a lighthouse.',\n",
        "         max_length=128)"
      ],
      "metadata": {
        "id": "3hoon3WAFeMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generate('Write a detailed analogy between mathematics and a music.',\n",
        "         max_length=256)"
      ],
      "metadata": {
        "id": "TrwFTMCYFeOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generate('What is the difference between a Llama, Vicuna and an Alpaca?',\n",
        "         max_length=512)"
      ],
      "metadata": {
        "id": "-ymIE3SVTvyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generate('Write a short email to Sam Altman giving reasons to open source GPT-4',\n",
        "         max_length=512)"
      ],
      "metadata": {
        "id": "IzQRjct_prr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generate('What is the capital of England?',\n",
        "         max_length=256)"
      ],
      "metadata": {
        "id": "YXZHQ0v3Tv0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "generate('Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.',\n",
        "         max_length=256)"
      ],
      "metadata": {
        "id": "TnGbQ7iU0XDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate('Write a story about a Koala playing pool and beating all the camelids.',\n",
        "         max_length=512)"
      ],
      "metadata": {
        "id": "_LJnsjfNTv4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat"
      ],
      "metadata": {
        "id": "puVWnDKrLRzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"\"\"Alice: I don't know why, I'm struggling to maintain focus while studying. Any suggestion? \\n\\n Bob:\"\"\",\n",
        "         max_length=128)"
      ],
      "metadata": {
        "id": "a_Qim0zcGWm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"\"\"Alice: I don't know why, I'm struggling to maintain focus while studying. Any suggestion? \\n\\n Bob:\"\"\",\n",
        "         max_length=128)"
      ],
      "metadata": {
        "id": "tH7QwHtpCTko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GSM8K"
      ],
      "metadata": {
        "id": "wnLzgM_dQDVm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FYI5JVn0QAXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate('Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?',\n",
        "         max_length=256)"
      ],
      "metadata": {
        "id": "rVJii8iRQG65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\",\n",
        "         max_length=128)"
      ],
      "metadata": {
        "id": "uM8iY879QJ66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"A deep-sea monster rises from the waters once every hundred years to feast on a ship and sate its hunger. Over three hundred years, it has consumed 847 people. Ships have been built larger over time, so each new ship has twice as many people as the last ship. How many people were on the ship the monster ate in the first hundred years?\",\n",
        "         max_length=256)"
      ],
      "metadata": {
        "id": "u-3V9dvQQezx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uAPnbMMeiVEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference:\n",
        "# https://colab.research.google.com/drive/1quYAZ5Od4PtwfhPjXmMI1dwyg1eLTN_h\n",
        "# https://colab.research.google.com/drive/1TWOC6waBUk7SY3T2hQi05skrTaZ7V13W?source=post_page-----129fa5e9a04d--------------------------------#scrollTo=-Dwb8zbHjdbN\n"
      ],
      "metadata": {
        "id": "K0a2Pf3viVQV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8819a475db2045d2b3d8dac242919801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d783bc4c8214eae8d00bbafb5b15648",
              "IPY_MODEL_70f960eb0b274ae9af704ca5954817af",
              "IPY_MODEL_4c4e027df1e8417596df3ce8c0fd7607"
            ],
            "layout": "IPY_MODEL_bfd29ef04ff84097911292dbe9199ced",
            "tabbable": null,
            "tooltip": null
          }
        },
        "9d783bc4c8214eae8d00bbafb5b15648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_99259e26917d4451989b3b657938f37f",
            "placeholder": "​",
            "style": "IPY_MODEL_a979263055b546368462af1d7b4c4cd8",
            "tabbable": null,
            "tooltip": null,
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "70f960eb0b274ae9af704ca5954817af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_26b9986c4ca84ac0b5e756958d1704ca",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3afa8942eec04e36b2512fd2f767c175",
            "tabbable": null,
            "tooltip": null,
            "value": 2
          }
        },
        "4c4e027df1e8417596df3ce8c0fd7607": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_47ddc739aaba4764969106545ffbb259",
            "placeholder": "​",
            "style": "IPY_MODEL_c26080549c0c474a890f5d1cdce9fe21",
            "tabbable": null,
            "tooltip": null,
            "value": " 2/2 [00:07&lt;00:00,  3.41s/it]"
          }
        },
        "bfd29ef04ff84097911292dbe9199ced": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99259e26917d4451989b3b657938f37f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a979263055b546368462af1d7b4c4cd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "26b9986c4ca84ac0b5e756958d1704ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3afa8942eec04e36b2512fd2f767c175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47ddc739aaba4764969106545ffbb259": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c26080549c0c474a890f5d1cdce9fe21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}